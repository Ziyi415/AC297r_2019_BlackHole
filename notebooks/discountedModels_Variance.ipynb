{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline model, we calculate the reward function for each telescope on each day using $$f(D, R) = -\\frac{1}{T(R)}\\sum_{t = 1}^{T(R)}{\\tau_{225}(D, t)},$$ where $D$ is the date we are looking at for the weather forecast, $R$ refers to the specific telescope, and $T(R)$ is the number of times forecasts being made in the telescope's observation timeframe. For each day, we calculate the day's reward by combining the telescopes' rewards in weighted average. $$F(D) = \\sum_{i = 1}^N{W_{R_i}\\times f(D, R_i)}.$$ Then we make decisions on whether to trigger the day based on the pure values of $F(D)$.\n",
    "\n",
    "In the discounted (punishment for uncertainty) model, we calculate $$F(D, r) = {\\sum_{i = 1}^N{W_{R_i}\\times f(D, R_i)}}\\times{(1+r)^D},$$ where $r$ is the punish level and then make decisions on $F(D,r)$. We multiply rather than divide here because the value is negative. We are going to experiment with different fixed value of $r$, and a function of $r$ depending on the variances in the forecasts, and compare the results with the ground-truth optimal path.\n",
    "\n",
    "We also experimented with different weights. In the baseline model, we were using the output  file size as a measure of how important each telescope is. In the discounted model, we adjusted the weights by using the area of the telescope. We further scaled all $F(D, r)$ to be a weighted average of $f(D, R_i)$, by letting $W_{R_i}$ be the proportion of telescope $R_i$'s area in the total area. By doing this, we think the output can be more interpretable.\n",
    "\n",
    "When we do evaluations, we are essentially looking backwards. Therefore, we directly calculate the scores without the discount factor $r$, for both the ground-truth path and our suggested path. The score given to any path $P$ is going to be $$S(P) = \\frac{2}{1 + e^{- \\sum_{j=1}^{\\text{Days}}{F(D_{P,j})}}},$$ where $D_{P,j}$ is the $j$-th day selected by the path $P$. We have $2$ in the numerator, because the sigmoid function will give us values from 0 to 0.5 based on our negative $F$ values and we want to get scores from 0 to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "telescopes = ['12-meter','alma','apex','aste','iram','jcmt','lmt','sma','smt','spt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime(2019,10,3,6)\n",
    "endtime = datetime(2019,10,14,0) # not included\n",
    "timestamps = np.arange(starttime, endtime, \n",
    "                       timedelta(hours=6)).astype(datetime)\n",
    "databook = {}\n",
    "for ts in telescopes:\n",
    "    databook[ts] = dict.fromkeys(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ts in telescopes:\n",
    "#     for t in timestamps:\n",
    "#         filepath = \"data/\"+ ts +\"/\"+ t.strftime(\"%Y%m%d_%H:%M:%S\")\n",
    "#         try:\n",
    "#             df = pd.read_csv(filepath, delim_whitespace=True, skiprows = 1, header = None)\n",
    "#             df.columns = [\"date\", \"tau225\", \"Tb[k]\", \"pwv[mm]\", \"lwp[kg*m^-2]\",\"iwp[kg*m^-2]\",\"o3[DU]\"]\n",
    "#             df['date'] = pd.to_datetime(df['date'], format = \"%Y%m%d_%H:%M:%S\")\n",
    "#             databook[ts][t] = df\n",
    "#         except FileNotFoundError:\n",
    "#             databook[ts][t] = None\n",
    "# # databook is a dictionary of dictionaries of dataframes \n",
    "# # keys: telescope names\n",
    "# # values: dictionaries of dataframes for one telescope\n",
    "# # databook[telescope_name] is a dictionary of dataframes for one telescope\n",
    "# # keys: timestamps when the forecast is made\n",
    "# # values: forecast dataframe (None if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## fake data ###################\n",
    "actual_time_span = np.arange(datetime(2019,7,1,0), datetime(2019,10,22,6), \n",
    "                       timedelta(hours=6)).astype(datetime)\n",
    "\n",
    "i = 0\n",
    "for site in telescopes:\n",
    "    for t in timestamps:\n",
    "        actual_time = actual_time_span[i]\n",
    "        time_delta = t - actual_time\n",
    "        filepath = \"data/MaunaKea/\"+ actual_time.strftime(\"%Y%m%d_%H:%M:%S\")\n",
    "        df = pd.read_csv(filepath, delim_whitespace=True, skiprows = 1, header = None)\n",
    "        df.columns = [\"date\", \"tau225\", \"Tb[k]\", \"pwv[mm]\", \"lwp[kg*m^-2]\",\"iwp[kg*m^-2]\",\"o3[DU]\"]\n",
    "        df['date'] = pd.to_datetime(df['date'], format = \"%Y%m%d_%H:%M:%S\") + time_delta\n",
    "        databook[site][t] = df\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-meter 2019-10-03 12:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tau225</th>\n",
       "      <th>Tb[k]</th>\n",
       "      <th>pwv[mm]</th>\n",
       "      <th>lwp[kg*m^-2]</th>\n",
       "      <th>iwp[kg*m^-2]</th>\n",
       "      <th>o3[DU]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-03 12:00:00</td>\n",
       "      <td>0.14078</td>\n",
       "      <td>39.414</td>\n",
       "      <td>3.2865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-03 13:00:00</td>\n",
       "      <td>0.14125</td>\n",
       "      <td>39.575</td>\n",
       "      <td>3.2760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-03 14:00:00</td>\n",
       "      <td>0.14093</td>\n",
       "      <td>39.556</td>\n",
       "      <td>3.2474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>265.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-03 15:00:00</td>\n",
       "      <td>0.13977</td>\n",
       "      <td>39.343</td>\n",
       "      <td>3.2021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>265.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-03 16:00:00</td>\n",
       "      <td>0.13965</td>\n",
       "      <td>39.350</td>\n",
       "      <td>3.1803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date   tau225   Tb[k]  pwv[mm]  lwp[kg*m^-2]  iwp[kg*m^-2]  \\\n",
       "0 2019-10-03 12:00:00  0.14078  39.414   3.2865           0.0           0.0   \n",
       "1 2019-10-03 13:00:00  0.14125  39.575   3.2760           0.0           0.0   \n",
       "2 2019-10-03 14:00:00  0.14093  39.556   3.2474           0.0           0.0   \n",
       "3 2019-10-03 15:00:00  0.13977  39.343   3.2021           0.0           0.0   \n",
       "4 2019-10-03 16:00:00  0.13965  39.350   3.1803           0.0           0.0   \n",
       "\n",
       "   o3[DU]  \n",
       "0  264.41  \n",
       "1  264.04  \n",
       "2  265.37  \n",
       "3  265.87  \n",
       "4  266.10  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(telescopes[0],timestamps[1])\n",
    "(databook[telescopes[0]][timestamps[1]]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "core: \n",
    "- the larger the punishment level is, the larger the difference between smaller and larger variance (in terms of ratio)\n",
    "- when punishment_level = 0, all effect of variances should be 1.\n",
    "- variance can be both <1 and >1, so $\\text{variance}^{\\text{punish_level}}$ does not work.\n",
    "\n",
    "So far we use:\n",
    "$$e^{\\text{variance} * \\text{punish_level}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Variance v.s. date of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Variance v.s. number of days forward (ideally using historical data to prevent information leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns are the timestamps when predictions were made, indices are timestamps that're being predicted\n",
    "std_dict = {}\n",
    "for site in telescopes:\n",
    "    data_telescope = databook[site]\n",
    "    df_tau225 = pd.concat([data_telescope[t].set_index('date')['tau225'] for t in data_telescope if data_telescope[t] is not None],axis = 1)\n",
    "    df_tau225.columns = [t for t in data_telescope if data_telescope[t] is not None]\n",
    "\n",
    "    # use df_mask to record the number of days forward (index - col) for each prediction\n",
    "    df_mask = pd.DataFrame(index = df_tau225.index, columns = df_tau225.columns)\n",
    "    for col in df_tau225.columns:\n",
    "        df_mask[col] = (df_tau225.index - col).days\n",
    "\n",
    "    # get 'true' tau225 value, which is the closest prediction within a day\n",
    "    true_tau225 = pd.Series(index = df_tau225.index)\n",
    "    for idx in df_tau225.index:\n",
    "        values = df_tau225.loc[idx]\n",
    "        true_tau225[idx] = values.values[~values.isna()][-1]\n",
    "\n",
    "    # compute standard error v.s. # days forward\n",
    "    df_diff = (df_tau225.T - true_tau225).T\n",
    "    std = []\n",
    "    for i in range(1, df_mask.max().max()+1):\n",
    "        std.append((np.nanmean(df_diff[df_mask==i].values ** 2))**(1/2))\n",
    "    \n",
    "    std_dict[site] = std[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_reward(telescope_name, day_current_str, end_day_str, start_time, end_time, \\\n",
    "            use_as_evaluate = False, time_variance = False, furthure_variance = False, punish_level = 1):\n",
    "    '''\n",
    "    For the specified telescope, return a dataframe with two columns.\n",
    "    The first column tells the day in the day window between \n",
    "        day_current_str and end_day_str (inclusive).\n",
    "    The second column tells the average predicted tao225 given the day and the time window between\n",
    "        start_time and end_time (inclusive).\n",
    "    \n",
    "    '''\n",
    "    split_day_current = day_current_str.split('-')\n",
    "    split_day_end = end_day_str.split('-') # include this day\n",
    "    \n",
    "    day_current = datetime(int(split_day_current[0]),int(split_day_current[1]),int(split_day_current[2]),0)\n",
    "    day_end = datetime(int(split_day_end[0]),int(split_day_end[1]),int(split_day_end[2])+1,0)\n",
    "    \n",
    "    if not use_as_evaluate:\n",
    "        mask = [t < day_current for t in databook[telescope_name]]\n",
    "        t_valid = np.array([t for t in databook[telescope_name]])[mask]\n",
    "\n",
    "        df_all = pd.concat([databook[telescope_name][t] for t in t_valid], axis =0)\n",
    "    else:\n",
    "        df_all = pd.concat([databook[telescope_name][t] for t in databook[telescope_name]], axis =0)\n",
    "    \n",
    "    # day and time filter\n",
    "    df_all = df_all[(df_all.date >= day_current) & (df_all.date < day_end)]\n",
    "    df_all['day'] = df_all.date.apply(lambda x: str(x).split(' ')[0])\n",
    "    df_all['time'] = df_all.date.apply(lambda x: int(str(x).split(' ')[1][0:2]))\n",
    "    df_all = df_all[(df_all.time >= int(start_time)) & (df_all.time <= int(end_time))]\n",
    "    \n",
    "    # calculate the day reward \n",
    "    df_tau_all = df_all.groupby('date').agg({'tau225':lambda x: list(x)}).reset_index()\n",
    "    df_tau_all['day'] = df_tau_all.date.apply(lambda x: str(x).split(' ')[0])\n",
    "    \n",
    "    df_tau_all['latest'] = df_tau_all['tau225'].apply(lambda x: x[-1]) \n",
    "    df_tau_all['mean'] = df_tau_all['tau225'].apply(lambda x: - np.mean(x)) \n",
    "    df_tau_all['std'] = df_tau_all['tau225'].apply(lambda x: np.std(x))\n",
    "    \n",
    "    if time_variance:\n",
    "        df_tau_day = pd.DataFrame(df_tau_all.groupby('day').apply(lambda x: np.mean(x['mean'] * np.exp(punish_level * x['std']))))\n",
    "        df_tau_day.columns = ['value']\n",
    "    \n",
    "    else:\n",
    "        df_tau_day = pd.DataFrame(-df_tau_all.groupby('day')['latest'].mean())\n",
    "        df_tau_day.columns = ['value']\n",
    "\n",
    "        if furthure_variance:\n",
    "            df_tau_day['value'] = df_tau_day['value'].values * np.exp(np.array(std_dict[telescope_name][:len(df_tau_day)]) * punish_level)  \n",
    "        \n",
    "    return df_tau_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted sum the reward for each telescope according to the total Gbytes.** \n",
    "( still need the suggested schedule for '12-meter','aste','iram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_telescope = [0, 22830.7, 26153.8, 0, 0, 12123.0, 22215.3, 12123.0, 18030.7, 26953.8]\n",
    "\n",
    "# using the area (radius ** 2) of the telescope as weights \n",
    "weight_telescope = [12**2, 73**2, 12**2, 10**2, 30**2, 15**2, 32.5**2, 14.7**2, 10**2, 6**2]\n",
    "# schedule_telescope = [[0,1], [3,13], [3,15], [0,1], [0,1], [10,16], [6,16], [10,16], [8,16], [3,15]]\n",
    "schedule_telescope = [[0,23], [3,13], [3,15], [0,23], [0,23], [10,16], [6,16], [10,16], [8,16], [3,15]]\n",
    "\n",
    "\n",
    "dict_schedule = dict(zip(telescopes, schedule_telescope))\n",
    "dict_weight = dict(zip(telescopes, weight_telescope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_day_reward(day_current_str, end_day_str, time_variance = False, furthure_variance = False, punish_level = 1):\n",
    "    \"\"\"\n",
    "    calculate F(D) for D in range(day_current_str, end_day_str)\n",
    "    taking in every single telescope we currently have\n",
    "    weighted their f reward values\n",
    "    based on area_i/total_area\n",
    "    \"\"\"\n",
    "    # set up a dataframe\n",
    "    telescopes_day_reward = day_reward(telescopes[0], day_current_str, end_day_str, \\\n",
    "                                       dict_schedule[telescopes[0]][0], dict_schedule[telescopes[0]][1], \\\n",
    "                                       time_variance = time_variance, furthure_variance = furthure_variance, punish_level = punish_level) \\\n",
    "                                       * dict_weight[telescopes[0]] \n",
    "    # \n",
    "    for i in telescopes[1:]:\n",
    "        telescopes_day_reward += day_reward(i, day_current_str, end_day_str, \\\n",
    "                                            dict_schedule[i][0], dict_schedule[i][1], \\\n",
    "                                            time_variance = time_variance, furthure_variance = furthure_variance,  punish_level = punish_level)\\\n",
    "                                            * dict_weight[i] \n",
    "    return telescopes_day_reward / sum(weight_telescope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Punish Level: Making Suggestions On-the-Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_making_single_punishment(day_current_str, end_day_str, days_to_trigger, punish_level = 0):\n",
    "    # day_current_str: YYYY-MM-DD (str) (included)\n",
    "    # end_day_str: YYYY-MM-DD (str) (included)\n",
    "    # days_to_trigger: days to trigger (int)\n",
    "    each_day_reward = all_day_reward(day_current_str, end_day_str)\n",
    "    \n",
    "    # inflate the values on each day\n",
    "    a = np.array([n * ((1 + punish_level) ** i) for i, n in enumerate(each_day_reward['value'])])\n",
    "    \n",
    "    # select the 'days_to_trigger' number of days having maximum reward values\n",
    "    selected_days = np.array(each_day_reward.index)[np.argsort(a)[-1:-days_to_trigger-1:-1]]\n",
    "    if day_current_str in selected_days:\n",
    "        print('We suggest triggering on today {}'.format(day_current_str))\n",
    "        output = True\n",
    "    else: \n",
    "        print('We DO NOT suggest triggering on today {}'.format(day_current_str))\n",
    "        output = False\n",
    "    print('And we suggest to trigger by the following sequence: {}'.format(np.array(sorted(selected_days))))\n",
    "    dic = dict(zip(each_day_reward.index, a))\n",
    "    print('The discounted reward values for all the future days are ', dic)\n",
    "    return output\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punish Level According To Variance (how far it is predicting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_making_further_variance_punishment(day_current_str, end_day_str, days_to_trigger, punish_level = 0):\n",
    "    # day_current_str: YYYY-MM-DD (str) (included)\n",
    "    # end_day_str: YYYY-MM-DD (str) (included)\n",
    "    # days_to_trigger: days to trigger (int)\n",
    "    each_day_reward = all_day_reward(day_current_str, end_day_str, furthure_variance = True, punish_level = punish_level)\n",
    "\n",
    "    # select the 'days_to_trigger' number of days having maximum reward values\n",
    "    selected_days = each_day_reward.sort_values(by='value', ascending=False)[:days_to_trigger].index\n",
    "    \n",
    "    if day_current_str in selected_days:\n",
    "        print('We suggest triggering on today {}'.format(day_current_str))\n",
    "        output = True\n",
    "    else: \n",
    "        print('We DO NOT suggest triggering on today {}'.format(day_current_str))\n",
    "        output = False\n",
    "    print('And we suggest to trigger by the following sequence: {}'.format(np.array(sorted(selected_days))))\n",
    "    dic = each_day_reward.to_dict()['value']\n",
    "    print('The discounted reward values for all the future days are ', dic)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punish Level According To Variance (the predictions for a specific time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_making_time_variance_punishment(day_current_str, end_day_str, days_to_trigger, punish_level = 0):\n",
    "    # day_current_str: YYYY-MM-DD (str) (included)\n",
    "    # end_day_str: YYYY-MM-DD (str) (included)\n",
    "    # days_to_trigger: days to trigger (int)\n",
    "    each_day_reward = all_day_reward(day_current_str, end_day_str, time_variance = True, punish_level = punish_level)\n",
    "\n",
    "    # select the 'days_to_trigger' number of days having maximum reward values\n",
    "    selected_days = each_day_reward.sort_values(by='value', ascending=False)[:days_to_trigger].index\n",
    "    \n",
    "    if day_current_str in selected_days:\n",
    "        print('We suggest triggering on today {}'.format(day_current_str))\n",
    "        output = True\n",
    "    else: \n",
    "        print('We DO NOT suggest triggering on today {}'.format(day_current_str))\n",
    "        output = False\n",
    "    print('And we suggest to trigger by the following sequence: {}'.format(np.array(sorted(selected_days))))\n",
    "    dic = each_day_reward.to_dict()['value']\n",
    "    print('The discounted reward values for all the future days are ', dic)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_process(days, num_days_trigger, function, punish_level):\n",
    "    decisions = []\n",
    "    for curr_day in days:\n",
    "        days_left = int(num_days_trigger - np.sum(decisions))\n",
    "        if days_left == 0:\n",
    "            pass\n",
    "        else:\n",
    "            decisions.append(\\\n",
    "                           function(curr_day, days[-1], days_left, punish_level))\n",
    "        print(\"\")\n",
    "    \n",
    "    suggested_path = days[decisions + [False] * (len(days) - len(decisions))]\n",
    "    return suggested_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_path_afterwards(start_day_str, end_day_str, days_to_trigger, exp_conversion, days_have_triggered = None):\n",
    "    # start_day_str: YYYY-MM-DD (str) (included)\n",
    "    # end_day_str: YYYY-MM-DD (str) (included)\n",
    "    # days_to_trigger: days to trigger (int)\n",
    "    # exp_conversion: function that map the negative value to [0, 1]\n",
    "    # days_have_triggered: days acutally triggered (list of str)\n",
    "    telescopes_day_reward = day_reward(telescopes[0], start_day_str, end_day_str, dict_schedule[telescopes[0]][0], dict_schedule[telescopes[0]][1], use_as_evaluate=True) * dict_weight[telescopes[0]]\n",
    "    for i in telescopes[1:]:\n",
    "        telescopes_day_reward += day_reward(i, start_day_str, end_day_str, dict_schedule[i][0], dict_schedule[i][1], use_as_evaluate=True) * dict_weight[i]\n",
    "    telescopes_day_reward = telescopes_day_reward / sum(weight_telescope)\n",
    "    \n",
    "    all_path = telescopes_day_reward.sort_values(by='value', ascending = False)\n",
    "    best_path = all_path[:days_to_trigger]\n",
    "    random_path = np.random.choice(all_path.index, days_to_trigger, replace = False)\n",
    "    print('The suggested path we predicted on the go is      {}'\\\n",
    "         .format(days_have_triggered))    \n",
    "    print('The best path to trigger based on ground-truth is {}'\\\n",
    "          .format(np.array(sorted(best_path.index))))\n",
    "    print('The random path to trigger is                     {}'\\\n",
    "          .format(np.array(sorted(random_path))))\n",
    "\n",
    "    if days_have_triggered is not None:\n",
    "        print('\\nThe score given to the suggested path is          {}'\\\n",
    "              .format(exp_conversion(all_path.loc[days_have_triggered]['value'])))\n",
    "    print('The score given to the best path is               {}'\\\n",
    "          .format(exp_conversion(best_path['value'])))\n",
    "    print('The score given to the random path is             {}'\\\n",
    "          .format(exp_conversion(all_path.loc[random_path]['value'])))\n",
    "\n",
    "    return all_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We suggest triggering on today 2019-10-05\n",
      "And we suggest to trigger by the following sequence: ['2019-10-05' '2019-10-06' '2019-10-07' '2019-10-08' '2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-05': -0.1444145363905571, '2019-10-06': -0.12534579854967196, '2019-10-07': -0.17935825364206198, '2019-10-08': -0.16590859152256887, '2019-10-09': -0.35755155652695064, '2019-10-10': -0.32807848389134914, '2019-10-11': -0.3338484696338396, '2019-10-12': -0.3445681334921599, '2019-10-13': -0.227712445596289, '2019-10-14': -0.24744430748626783}\n",
      "\n",
      "We suggest triggering on today 2019-10-06\n",
      "And we suggest to trigger by the following sequence: ['2019-10-06' '2019-10-07' '2019-10-08' '2019-10-09']\n",
      "The discounted reward values for all the future days are  {'2019-10-06': -0.13931276546644886, '2019-10-07': -0.14624908553479665, '2019-10-08': -0.15100488032522855, '2019-10-09': -0.17712320866378195, '2019-10-10': -0.27047643621685113, '2019-10-11': -0.40737569067188784, '2019-10-12': -0.30728761533903604, '2019-10-13': -0.2427561565621859, '2019-10-14': -0.30323286488683054}\n",
      "\n",
      "We suggest triggering on today 2019-10-07\n",
      "And we suggest to trigger by the following sequence: ['2019-10-07' '2019-10-08' '2019-10-09']\n",
      "The discounted reward values for all the future days are  {'2019-10-07': -0.15280227218661957, '2019-10-08': -0.13711440275288703, '2019-10-09': -0.15671139269690493, '2019-10-10': -0.2528472115939743, '2019-10-11': -0.2938856086130518, '2019-10-12': -0.29037970243413863, '2019-10-13': -0.24988346405200618, '2019-10-14': -0.36182586831702057}\n",
      "\n",
      "We suggest triggering on today 2019-10-08\n",
      "And we suggest to trigger by the following sequence: ['2019-10-08' '2019-10-09']\n",
      "The discounted reward values for all the future days are  {'2019-10-08': -0.1443869321575379, '2019-10-09': -0.1328806923316089, '2019-10-10': -0.3081405001129143, '2019-10-11': -0.25242979108245345, '2019-10-12': -0.20929667518863868, '2019-10-13': -0.26564062444430875, '2019-10-14': -0.2936644776615445}\n",
      "\n",
      "We suggest triggering on today 2019-10-09\n",
      "And we suggest to trigger by the following sequence: ['2019-10-09']\n",
      "The discounted reward values for all the future days are  {'2019-10-09': -0.12110789810272407, '2019-10-10': -0.20513266692467286, '2019-10-11': -0.25094122893976817, '2019-10-12': -0.19237376838580392, '2019-10-13': -0.15160630204772274, '2019-10-14': -0.21271065727026356}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whole_time_window = all_day_reward('2019-10-05', '2019-10-14')\n",
    "days = np.array(whole_time_window.index)\n",
    "suggested_path = simulate_process(days, 5, decision_making_single_punishment, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We suggest triggering on today 2019-10-05\n",
      "And we suggest to trigger by the following sequence: ['2019-10-05' '2019-10-06' '2019-10-08' '2019-10-13' '2019-10-14']\n",
      "The discounted reward values for all the future days are  {'2019-10-05': -0.14693705855562592, '2019-10-06': -0.11732646609949775, '2019-10-07': -0.15206425273199173, '2019-10-08': -0.12780026495811003, '2019-10-09': -0.25260675117528486, '2019-10-10': -0.20907876739941544, '2019-10-11': -0.19326507895314232, '2019-10-12': -0.18210285545953298, '2019-10-13': -0.10973242972044575, '2019-10-14': -0.10833626978808757}\n",
      "\n",
      "We DO NOT suggest triggering on today 2019-10-06\n",
      "And we suggest to trigger by the following sequence: ['2019-10-07' '2019-10-08' '2019-10-09' '2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-06': -0.1415951828762349, '2019-10-07': -0.1369674705337123, '2019-10-08': -0.12793406274440994, '2019-10-09': -0.1368887956479974, '2019-10-10': -0.1898414263287113, '2019-10-11': -0.2596158827552171, '2019-10-12': -0.17794968543920323, '2019-10-13': -0.12803401903045436, '2019-10-14': -0.14627187931549437}\n",
      "\n",
      "We suggest triggering on today 2019-10-07\n",
      "And we suggest to trigger by the following sequence: ['2019-10-07' '2019-10-08' '2019-10-09' '2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-07': -0.1552841335264251, '2019-10-08': -0.12826395780580702, '2019-10-09': -0.13273043169339047, '2019-10-10': -0.19486003016535666, '2019-10-11': -0.20628987219358724, '2019-10-12': -0.1850317763578321, '2019-10-13': -0.14473910222963327, '2019-10-14': -0.1909656937965991}\n",
      "\n",
      "We suggest triggering on today 2019-10-08\n",
      "And we suggest to trigger by the following sequence: ['2019-10-08' '2019-10-09' '2019-10-12']\n",
      "The discounted reward values for all the future days are  {'2019-10-08': -0.14679974369628904, '2019-10-09': -0.12431428269976134, '2019-10-10': -0.261543294523935, '2019-10-11': -0.19462946080759938, '2019-10-12': -0.14693025888942604, '2019-10-13': -0.16936906266480106, '2019-10-14': -0.17013742124452502}\n",
      "\n",
      "We suggest triggering on today 2019-10-09\n",
      "And we suggest to trigger by the following sequence: ['2019-10-09' '2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-09': -0.1230567416274562, '2019-10-10': -0.1923309494324541, '2019-10-11': -0.21255604032179856, '2019-10-12': -0.14840563078844554, '2019-10-13': -0.10654717078706603, '2019-10-14': -0.13546750378513883}\n",
      "\n",
      "We DO NOT suggest triggering on today 2019-10-10\n",
      "And we suggest to trigger by the following sequence: ['2019-10-14']\n",
      "The discounted reward values for all the future days are  {'2019-10-10': -0.18544195455722493, '2019-10-11': -0.21847686061626656, '2019-10-12': -0.17206333032541551, '2019-10-13': -0.14177667244430378, '2019-10-14': -0.13149432203343722}\n",
      "\n",
      "We DO NOT suggest triggering on today 2019-10-11\n",
      "And we suggest to trigger by the following sequence: ['2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-11': -0.2424781993046357, '2019-10-12': -0.1664179908937106, '2019-10-13': -0.11392164956857055, '2019-10-14': -0.38435722960887203}\n",
      "\n",
      "We suggest triggering on today 2019-10-12\n",
      "And we suggest to trigger by the following sequence: ['2019-10-12']\n",
      "The discounted reward values for all the future days are  {'2019-10-12': -0.15579968240396216, '2019-10-13': -0.1661481954197546, '2019-10-14': -1.3107534940951497}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whole_time_window = all_day_reward('2019-10-05', '2019-10-14')\n",
    "days = np.array(whole_time_window.index)\n",
    "suggested_path = simulate_process(days, 5, decision_making_further_variance_punishment, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We suggest triggering on today 2019-10-05\n",
      "And we suggest to trigger by the following sequence: ['2019-10-05' '2019-10-06' '2019-10-07' '2019-10-08' '2019-10-11']\n",
      "The discounted reward values for all the future days are  {'2019-10-05': -0.13243849904403848, '2019-10-06': -0.12161995065761698, '2019-10-07': -0.16921350192269902, '2019-10-08': -0.16373792909290136, '2019-10-09': -0.2791885738557433, '2019-10-10': -0.22126367695605662, '2019-10-11': -0.18606743009489204, '2019-10-12': -0.19107653737175037, '2019-10-13': -0.18698780188877803, '2019-10-14': -0.19071616842195252}\n",
      "\n",
      "We suggest triggering on today 2019-10-06\n",
      "And we suggest to trigger by the following sequence: ['2019-10-06' '2019-10-07' '2019-10-08' '2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-06': -0.12481163794562951, '2019-10-07': -0.16052099552991256, '2019-10-08': -0.1496183405254709, '2019-10-09': -0.21736731570394638, '2019-10-10': -0.19994000654901548, '2019-10-11': -0.2000036040230988, '2019-10-12': -0.177245115499339, '2019-10-13': -0.17161691764098164, '2019-10-14': -0.1776738257821026}\n",
      "\n",
      "We suggest triggering on today 2019-10-07\n",
      "And we suggest to trigger by the following sequence: ['2019-10-07' '2019-10-08' '2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-07': -0.15822337822413224, '2019-10-08': -0.14155322659035816, '2019-10-09': -0.18595930940192187, '2019-10-10': -0.19785513403746136, '2019-10-11': -0.21064846725957165, '2019-10-12': -0.17416279654146022, '2019-10-13': -0.16295000798519818, '2019-10-14': -0.17649251111272402}\n",
      "\n",
      "We suggest triggering on today 2019-10-08\n",
      "And we suggest to trigger by the following sequence: ['2019-10-08' '2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-08': -0.14137281816494998, '2019-10-09': -0.169967206835597, '2019-10-10': -0.20584396528599702, '2019-10-11': -0.20701508227703727, '2019-10-12': -0.1613596971008722, '2019-10-13': -0.16109185732594772, '2019-10-14': -0.1771483641297254}\n",
      "\n",
      "We DO NOT suggest triggering on today 2019-10-09\n",
      "And we suggest to trigger by the following sequence: ['2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-09': -0.15922614893917247, '2019-10-10': -0.20677571270668543, '2019-10-11': -0.20545907964180413, '2019-10-12': -0.15278439684313253, '2019-10-13': -0.12277548951228029, '2019-10-14': -0.1736273148153827}\n",
      "\n",
      "We DO NOT suggest triggering on today 2019-10-10\n",
      "And we suggest to trigger by the following sequence: ['2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-10': -0.20165105308031936, '2019-10-11': -0.20748550138854818, '2019-10-12': -0.15963884316552807, '2019-10-13': -0.12479472472333589, '2019-10-14': -0.14283833453251954}\n",
      "\n",
      "We DO NOT suggest triggering on today 2019-10-11\n",
      "And we suggest to trigger by the following sequence: ['2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-11': -0.21080202803850864, '2019-10-12': -0.16627541724481473, '2019-10-13': -0.12842293432736163, '2019-10-14': -0.18301343930849745}\n",
      "\n",
      "We DO NOT suggest triggering on today 2019-10-12\n",
      "And we suggest to trigger by the following sequence: ['2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-12': -0.1644221722648542, '2019-10-13': -0.13071721139556206, '2019-10-14': -0.35238389602244286}\n",
      "\n",
      "We suggest triggering on today 2019-10-13\n",
      "And we suggest to trigger by the following sequence: ['2019-10-13']\n",
      "The discounted reward values for all the future days are  {'2019-10-13': -0.13433143032285585, '2019-10-14': -0.4565316347166931}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whole_time_window = all_day_reward('2019-10-05', '2019-10-14')\n",
    "days = np.array(whole_time_window.index)\n",
    "suggested_path = simulate_process(days, 5, decision_making_time_variance_punishment, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_function = lambda x: 2 / (1 + np.exp(- x.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The suggested path we predicted on the go is      ['2019-10-05' '2019-10-06' '2019-10-07' '2019-10-08' '2019-10-13']\n",
      "The best path to trigger based on ground-truth is ['2019-10-06' '2019-10-07' '2019-10-08' '2019-10-09' '2019-10-12']\n",
      "The random path to trigger is                     ['2019-10-06' '2019-10-07' '2019-10-12' '2019-10-13' '2019-10-14']\n",
      "\n",
      "The score given to the suggested path is          0.6424974899928185\n",
      "The score given to the best path is               0.6777753688078361\n",
      "The score given to the random path is             0.5368666688988142\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "all_state = best_path_afterwards('2019-10-05', '2019-10-14', 5, exp_function, days_have_triggered = suggested_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
